{
  "prompt": "I have a scanned PDF document that's just images of text - I need to extract the actual text content from it",
  "baseline_response": "To extract text from a scanned PDF document (which is essentially an image), you'll need to use Optical Character Recognition (OCR) technology. Here are several methods:\n\n1. Online Tools:\n- Google Drive OCR\n- Adobe Acrobat Pro\n- Microsoft OneNote\n- Online OCR websites (PDF2Go, OCR.Space)\n\n2. Desktop Software:\n- ABBYY FineReader\n- Tesseract (open-source)\n- Adobe Acrobat Pro\n- OmniPage\n- Microsoft Office Lens\n\n3. Programming Libraries:\n- Python: \n  - pytesseract\n  - pdf2image\n  - Pillow\n- Tesseract OCR\n- OpenCV\n\n4. Steps for basic Python OCR:\n```python\nimport pytesseract\nfrom pdf2image import convert_from_path\n\n# Convert PDF to images\npages = convert_from_path('document.pdf')\n\n# Extract text from each page\nfor page in pages:\n    text = pytesseract.image_to_string(page)\n    print(text)\n```\n\nChoose method based on:\n- Budget\n- Volume of documents\n- Technical skills\n- Accuracy requirements\n\nWould you like me to elaborate on any of these approaches?",
  "skill_response": "Here's a comprehensive approach to extract text from a scanned PDF using Optical Character Recognition (OCR):\n\n```python\n# Required libraries\nimport pytesseract\nfrom pdf2image import convert_from_path\nimport cv2\nimport numpy as np\n\ndef preprocess_image(image):\n    \"\"\"\n    Preprocess image to improve OCR accuracy\n    \"\"\"\n    # Convert to grayscale\n    gray = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2GRAY)\n    \n    # Threshold to remove noise\n    thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]\n    \n    # Optional: Denoise\n    denoised = cv2.fastNlMeansDenoising(thresh)\n    \n    return denoised\n\ndef extract_text_from_scanned_pdf(pdf_path, lang='eng'):\n    \"\"\"\n    Extract text from a scanned PDF\n    \n    :param pdf_path: Path to the PDF file\n    :param lang: Language for OCR (default English)\n    :return: Extracted text\n    \"\"\"\n    # Convert PDF to images\n    try:\n        images = convert_from_path(pdf_path)\n    except Exception as e:\n        print(f\"Error converting PDF to images: {e}\")\n        return \"\"\n    \n    # Configure Tesseract\n    pytesseract.pytesseract.tesseract_cmd = r'/usr/bin/tesseract'  # Adjust path as needed\n    \n    # Extract text from each page\n    full_text = \"\"\n    for i, image in enumerate(images):\n        # Preprocess image\n        preprocessed_image = preprocess_image(image)\n        \n        # Perform OCR\n        try:\n            page_text = pytesseract.image_to_string(\n                preprocessed_image, \n                lang=lang,\n                config='--psm 6'  # Assume a single uniform block of text\n            )\n            \n            full_text += f\"--- Page {i+1} ---\\n\"\n            full_text += page_text + \"\\n\\n\"\n        \n        except Exception as e:\n            print(f\"Error processing page {i+1}: {e}\")\n    \n    return full_text\n\n# Advanced usage with multiple language support\ndef extract_multilingual_text(pdf_path, languages=['eng', 'fra', 'deu']):\n    \"\"\"\n    Extract text with multiple language support\n    \"\"\"\n    # Combine languages\n    lang_string = '+'.join(languages)\n    \n    # Convert PDF to images\n    images = convert_from_path(pdf_path)\n    \n    full_text = \"\"\n    for i, image in enumerate(images):\n        preprocessed_image = preprocess_image(image)\n        \n        page_text = pytesseract.image_to_string(\n            preprocessed_image, \n            lang=lang_string,\n            config='--psm 6'\n        )\n        \n        full_text += f\"--- Page {i+1} ---\\n\"\n        full_text += page_text + \"\\n\\n\"\n    \n    return full_text\n\n# Example usage\ndef main():\n    pdf_path = 'scanned_document.pdf'\n    \n    # Basic extraction\n    extracted_text = extract_text_from_scanned_pdf(pdf_path)\n    \n    # Save to file\n    with open('extracted_text.txt', 'w', encoding='utf-8') as f:\n        f.write(extracted_text)\n    \n    print(\"Text extraction complete!\")\n\n# Additional error handling and logging\nimport logging\n\ndef setup_logging():\n    logging.basicConfig(\n        level=logging.INFO,\n        format='%(asctime)s - %(levelname)s: %(message)s',\n        filename='ocr_log.txt'\n    )\n\n# Run the script\nif __name__ == '__main__':\n    setup_logging()\n    main()\n```\n\n## Prerequisites\n\n1. Install required libraries:\n```bash\npip install pytesseract pdf2image opencv-python-headless\n```\n\n2. Install Tesseract OCR:\n- Ubuntu/Debian: `sudo apt-get install tesseract-ocr`\n- macOS: `brew install tesseract`\n- Windows: Download from official Tesseract GitHub\n\n3. Download language data:\n```bash\n# For additional languages\nsudo apt-get install tesseract-ocr-[lang]\n# e.g., tesseract-ocr-fra for French\n```\n\n## Advanced Tips\n\n### Improve OCR Accuracy\n1. Preprocessing techniques:\n   - Adjust brightness/contrast\n   - Remove background noise\n   - Correct skew\n   - Enhance image resolution\n\n```python\ndef advanced_preprocess(image):\n    # More aggressive preprocessing\n    gray = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2GRAY)\n    \n    # Adaptive thresholding\n    thresh = cv2.adaptiveThreshold(\n        gray, 255, \n        cv2.ADAPTIVE_THRESH_GAUSSIAN_C, \n        cv2.THRESH_BINARY, 11, 2\n    )\n    \n    # Deskew\n    coords = np.column_stack(np.where(thresh > 0))\n    angle = cv2.minAreaRect(coords)[-1]\n    \n    # Rotate to correct skew\n    (h, w) = image.shape[:2]\n    center = (w // 2, h // 2)\n    M = cv2.getRotationMatrix2D(center, angle, 1.0)\n    rotated = cv2.warpAffine(\n        thresh, M, (w, h), \n        flags=cv2.INTER_CUBIC, \n        borderMode=cv2.BORDER_REPLICATE\n    )\n    \n    return rotated\n```\n\n### Handling Complex Documents\n- Use `--psm` (Page Segmentation Mode) options:\n  - 3: Fully automatic page segmentation\n  - 6: Assume a single uniform block of text\n  - 11: Sparse text. Find as many text lines as possible\n\n### Multilingual Support\n- Install language data packs\n- Specify multiple languages in extraction\n\n### Error Handling\n- Log OCR errors\n- Handle different image qualities\n- Provide fallback mechanisms\n\n## Troubleshooting\n1. Poor text extraction:\n   - Check image quality\n   - Experiment with preprocessing\n   - Adjust Tesseract configuration\n\n2. Performance issues:\n   - Reduce image resolution\n   - Process in batches\n   - Use multiprocessing\n\n## Performance Optimization\n```python\nfrom concurrent.futures import ProcessPoolExecutor\n\ndef parallel_ocr_extraction(pdf_path):\n    with ProcessPoolExecutor() as executor:\n        images = convert_from_path(pdf_path)\n        futures = [\n            executor.submit(pytesseract.image_to_string, img) \n            for img in images\n        ]\n        \n        results = [future.result() for future in futures]\n    \n    return '\\n'.join(results)\n```\n\n## Recommended Workflow\n1. Preprocess images\n2. Apply OCR\n3. Post-process text (clean up)\n4. Validate and correct",
  "verdict": "skill",
  "reasoning": "Response A is significantly more comprehensive and practical. It provides a complete, working solution with detailed code implementation, preprocessing techniques for better OCR accuracy, error handling, multilingual support, and performance optimization. It includes proper documentation, troubleshooting tips, and advanced techniques like image deskewing and parallel processing. Response B, while covering multiple approaches, is much more surface-level and only provides a basic 4-line code snippet without proper implementation details, error handling, or optimization guidance. For someone who actually needs to extract text from scanned PDFs, Response A provides everything needed to implement a robust solution.",
  "baseline_tokens": 322,
  "skill_tokens": 4014,
  "position_a": "skill",
  "position_b": "baseline",
  "judge_model": "claude-sonnet-4-20250514",
  "judged_at": "2026-01-11T16:26:37.426410Z"
}